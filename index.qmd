## Santolin et al. (2024)

::: columns
:::: {.column width="80%"}
::::: incremental

- **Chunking the continuous speech stream** into relevant linguistic units (phonemes, syllables, words, phrases): major milestone in language acquisition
- Consensus: **syllables are advantaged** [@bijeljac19934; @bertoncini1995morae]
- From birth, infants can discriminate between syllables, based on their internal structure (e.g., CV, VC, CVC, CCV).

:::::
::::
:::

## Syllable chunking and structure classification: mechanisms?

Language universals as *Gates to Language* (GaLa):

* **Sonority Sequencing Principle** (SSP): sonority peaks at syllable nucleus.
* **Maximal Onset Principle** (MOP): consonants are preferably grouped at the onset of a syllable instead of coda.

## Sonority Sequencing Principle (SSP)

![Sonority Sequencing Principle (SSP). Figure from Santolin et al., (2024).](img/ssp.PNG)

## Sonority Sequencing Principle (SSP)

::: columns
:::: {.column width="80%"}

Experimental series involving:

* Neonates (fNIRS)
* Infants (HPP)
* Long-Evans rats (behavioural)
* Artificial Recurrent Neural Networks (**RNNs**)

::::
:::: {.column}
::::
:::

## Santolin et al. (2024)

::: box
Do infant *encode*-*generalize* internal structure of syllables?
:::

::: columns
:::: {.column width="30%"}

**Head-turn Preference Procedure**

::::

:::: {.column width="35%"}
*Familiarization phase*

|CVC|CCV|
|---|---|
|sam|sma|
|gel|gle|
|pus|psu|
|dor|dro|
|sen|sne|

::::
:::: {.column width="35%"}
*Test phase*

|CVC|CCV|
|---|---|
|sap|spa|
|kos|kso|

::::
:::

## Santolin et al. (2024)

::: columns
:::: {.column}
- Infants encode and generalise CVC and CCV syllabic structures **only when familiarized with CVC**
- Generalisation occurs **regardless of phonetic information**
::::
:::: {.column}
![](img/santolin2024.jpg){width=90%}
::::
:::

## Research question

::: columns
:::: {.column width="70%"}
::::: incremental
::::: box
Are CCV syllables more **difficult** to process than CVC syllables?
:::::

Simulate experimental outputs using **Recurrent Neural Networks (RNN)**

:::::
::::
:::: {.column}
::::
:::

## Neural Networks (NN)

::: columns
:::: {.column width="70%"}

- Bunch of regression models (nodes) stacked in layers
- Receive **input**, generate **output**
- Some nodes inform other nodes via connections whose relative importance is determined by *weights*

::::
:::: {.column}
::::
:::

## *Recurrent* Neural Networks (RNN)

::: columns
:::: {.column width="70%"}

- Account for arbitrarily unfolding **time series**
- Receive the additional input of their own previous state
- Routinely used in speech recognition, text processing (e.g., transformers), and speech generation software

::::
:::: {.column}
::::
:::

## Proof of concept

::: columns
:::: {.column width="70%"}

**Supervised audio classification task**

* Starting small: CV vs. VC syllables
* Keep the model as **simple** (i.e., interpretable) as possible

@magnuson2020earshot

::::
:::: {.column}
::::
:::

## Audio processing

::: columns
:::: {.column width="90%"}
::::: incremental

* 7,000 audios, 700 unique **syllables** $\times$ 10 speakers^[Apple Text-to-Speech, @magnuson2020earshot]
    - 3500 consonant-vowel (CV)
    - 3500 vowel-consonant (VC)
- Amplitude envelope [@deloche2024acoustic]
- Normalized amplitude and duration (downsampling) across audios 

:::::
::::
:::: {.column}
::::
:::

---

![](img/envelopes.png)

--- 

![](img/training-dataset.png)

## RNN structure

::: incremental

- **1 input node** (receiving one audio sample in each time step)
- **2x2 recurrent nodes**
- **1 output node** ($\sigma$ activation function), outputs a probability $\in [0, 1]$
    - $\approx 1$ more likely `CV`, $\approx 0$, more likely `VC`

:::

![](img/struct.png)

## RNN structure

- **1 input node** (receiving one audio sample in each time step)
- **2x2 recurrent nodes**
- **1 output node** ($\sigma$ activation function), outputs a probability $\in [0, 1]$
    - $\approx 1$ more likely `CV`, $\approx 0$, more likely `VC`

![](img/struct-2.png)


## Model training

::: columns
:::: {.column width="70%"}
::::: box

- Optimizer: `ADAM` ($\epsilon = 0.001$)
- Binary cross-entropy loss function
- 30 epochs (early stopping at 95% accuracy)
- Batch size: 16

:::::
::::
:::: {.column width="30%"}
![Tensorflow + Keras](img/tf.jpg)
::::
:::

(Still tweaking things around.)


# Results {.inverse}

## Preliminary results

![](img/metrics.png)

## Preliminary results

![](img/test-accuracy.png)

## Preliminary results

![](img/roc.png)

## Future steps

::: incremental

- Use **spectrograms** instead of envelope [e.g., @magnuson2020earshot]
- **Unsupervised learning**: replace output node with output layer (generation of spectrograms)
    - Encoder-decoder: What does the model think stereotipical CV or VC spectrograms look like?
- More complex syllable structures: **CVC, CCV**
- Take a look at *embeddings*

:::

## Future steps

Is the model better at classifying CVCs than CCVs? [@santolin2024abstract]


::: box
- *Yes*: **complexity** of the speech signal? (e.g., CC cluster)
- *No*:  infants have accumulated more **experience** with CVC (more frequent) than CCV?
    - If so, can we reproduce the results by manipulating the frequency of each syllabic structure in the model's input
:::




## References

